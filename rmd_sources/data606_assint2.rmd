---
title: "Assignment 2"
subtitle: "Logistic regression, LDA, QDA"
author:
  - Andrii Voitkiv
date: "`r format(Sys.time(), '%a, %b %d, %Y')`"
geometry: margin=3cm
output:
  github_document:
    html_preview: false
    math_method: webtex
    toc: true
    toc_depth: 2
---

```{r global_options}
knitr::opts_chunk$set(fig.path='Figs/')
```
<!-- title: "Assignment 2"
author: "Andrii Voitkiv"
output:
html_document:
toc: true
toc_float: true
number_sections: false
theme: united
highlight: tango
df_print: paged
self_contained: true
keep_md: true
toc_depth: 2 -->

## Question 1
_Read the data “PimaIndiansDiabetes”, check the dimension of the dataset, the names of variables. Check how many classes there are (the variable “Diabetes”) and the class sizes. Check how R dummifies the “diabetes” variable._
```{r}
# Read the data
library('mlbench')
data(PimaIndiansDiabetes)
# Check the dimension of the dataset
dim(PimaIndiansDiabetes)
# Check the names of variables
names(PimaIndiansDiabetes)
# Check how many classes there are (the variable “Diabetes”) and the class sizes
table(PimaIndiansDiabetes$diabetes)
```
The dataset has 768 observations and 9 variables.The names of variables are: pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age and diabetes. There are two classes in the variable diabetes: "neg" and "pos". The class sizes are 500 and 268, respectively.

## Question 2
_Split the dataset into two parts – training and test.The training part contains 400 individuals from the “neg” class and 200 units from the “pos” class. The test part contains the rest data._
```{r}
# Create vector of indices for "neg" and "pos" classes
neg_ix <- which(PimaIndiansDiabetes$diabetes == "neg")
pos_ix <- which(PimaIndiansDiabetes$diabetes == "pos")
# Randomly select 400 indices from "neg" class and 200 indices from "pos" class
set.seed(10)
train_neg_ix <- sample(neg_ix, 400)
train_pos_ix <- sample(pos_ix, 200)
# Combine the two vectors
train_ix <- c(train_neg_ix, train_pos_ix)
# Select the rest indices as the test indices
test_ix <- setdiff(1:dim(PimaIndiansDiabetes)[1], train_ix)
# Create the training and test datasets
train <- PimaIndiansDiabetes[train_ix,]
test <- PimaIndiansDiabetes[test_ix,]
```
_Let “diabetes” be the response variable, get a logistic regression model based on the training part using all the explanatory variables, which variables contribute to the “pos” result in a negative way?_
```{r}
# Get a logistic regression model based on the training part using all the explanatory variables
model <- glm(diabetes ~ ., data = train, family = binomial)
# Check the coefficients
summary(model)
```
```{r}
# Check encoding of the variable “diabetes”
contrasts(PimaIndiansDiabetes$diabetes)
```
Conclusion: Default encoding of the variable “diabetes” is 1 for "pos". There is one statistically _significant_ variable that contribute to the “pos” result in a negative way. And that is "pressure". From the non-significant variables, the variable insulin contribute to the “pos” result in a negative way.

_Check the “variance inflation factor” for each coefficient (e.g., using “vif” function from the package “car”). Do you detect multicollinearity?_
```{r correlation-matrix, warning = FALSE, message = FALSE}
# Plot the correlation matrix with the help of the ggally package
library('GGally')
library('ggplot2')
ggpairs(PimaIndiansDiabetes, lower = list(continuous = "smooth_loess", combo = "facethist", discrete = "facetbar", na = "na"))

```

```{r message=FALSE, warning=FALSE}
# Check the “variance inflation factor” for each coefficient using imcdiag function from the package "regclass"
library('regclass')
VIF(model)
```
The correlation matrix shows that there is no multicollinearity between the variables. The variance inflation factor is less than 5 for all the variables, so there is no multicollinearity.

## Question 3
_Apply your fitted logistic regression model to the test set. Suppose a prediction of “pos” is made if P(Y=pos|X1,…,Xp)≥0.5
, check the misclassification rate (the ratio between the number of incorrect predictions and test size)._
```{r}
# Apply the fitted logistic regression model to the test set
pred <- predict(model, test, type = "response")
# Check the misclassification rate
misclass_rate <- sum(ifelse(pred >= 0.5, 'pos', 'neg') != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate
```

## Question 4
_Let the significance level be 0.1, remove the explanatory variables whose coefficients are NOT significantly different from 0. Re-fit a logistic regression model and repeat Step 3 to calculate the misclassification rate. (3pt)_
```{r}
# Check the sificance level of the coefficients with the significance level 0.1
summary(model)
```
With the significance level 0.1, the coefficients of the variables `pregnant, glucose, pressure, mass and pedigree` are significantly different from 0. So, we will keep them in the model and will drop the rest.


```{r}
# Re-fit a logistic regression model
model2 <- glm(diabetes ~ pregnant + glucose + pressure + mass + pedigree, data = train, family = binomial)
# Check the coefficients
summary(model2)
```

The valid model's equation is:
```{r echo = FALSE}
# display the actual coefficients
equatiomatic::extract_eq(model2, use_coefs = TRUE)
```

```{r}
# Apply the fitted logistic regression model to the test set
pred2 <- predict(model2, test, type = "response")
# Check the misclassification rate
misclass_rate2 <- sum(ifelse(pred2 >= 0.5, 'pos', 'neg') != test$diabetes)/dim(test)[1]
# Print the misclassification rates
misclass_rate2
```
If we compare the misclassification rates of the two models, we can see that the misclassification rate of the second model is slightly lower than the misclassification rate of the first model. So, the second model is better than the first one.

## Question 5
_Under the same training and test sets, apply a linear discriminant analysis (LDA) model to the training set using all the explanatory variables. Apply this fitted LDA model to the test set, calculate the misclassification rate based on the LDA-predicted results and the actual results. (3pt)_
```{r}
# Apply a linear discriminant analysis (LDA) model to the training set using all the explanatory variables
library('MASS')
model3 <- lda(diabetes ~ ., data = train)
model3
```
```{r}
# Apply the fitted LDA model to the test set
pred3 <- predict(model3, test)
# Check the misclassification rate
misclass_rate3 <- sum(pred3$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate3
```

## Question 6
_Suppose now you only want to use two explanatory variables among “pregnant”, “glucose”, “mass” and “pedigree” to establish the LDA model. Which two variables can give you the lowest error rate based on the training set? After you get the LDA model, apply it to the test set, what is the misclassification rate? (3pt)_
```{r results = 'asis'}
# METHOD 1
# Check the misclassification rate for each pair of variables
variables <- c('pregnant', 'glucose', 'mass', 'pedigree')
misclass_result <- list(var1=c(), var2=c(), misclass_rate=c())

for (i in variables) {
  for (j in variables) {
    # Check if the pair is not the same
    # And if the pair is not already in the list
    if (i != j & !((i %in% misclass_result$var2) & (j %in% misclass_result$var1))) {
      pair <- c(i, j)
      model4 <- lda(reformulate(pair,"diabetes"), data = train)
      pred4 <- predict(model4, test)
      # Populate the list
      misclass_result$var1 <- c(misclass_result$var1, i)
      misclass_result$var2 <- c(misclass_result$var2, j)
      misclass_result$misclass_rate <- c(misclass_result$misclass_rate, sum(pred4$class != test$diabetes)/dim(test)[1])
    }
  }
}
# Transform the list to a data frame
misclass_result <- as.data.frame(misclass_result)
# Sort the data frame by the misclassification rate
misclass_result <- misclass_result[order(misclass_result$misclass_rate),]
# Print the data frame
library("knitr")
kable(misclass_result)
```
The lowest error rate is `r misclass_result$misclass_rate[1]`. And it is achieved by the pair of variables `r paste(misclass_result$var1[1], "and", misclass_result$var2[1], sep = " ")`.

```{r}
# METHOD 2
library('klaR')
partimat(diabetes ~ pregnant + glucose + mass + pedigree, data = train, method="lda")
```

Method 1 (when I plug different pairs of variables into the LDA model) gives different output than Method 2 (when I use the `partimat` function). I don't know why. Method 2 is a black box for me. And from my best understanding of the problem, I will use Method 1 to answer the question (because I understand what I'm doing there).
```{r}
# Build the LDA model with the pair of variables that gives the lowest error rate
model5 <- lda(diabetes ~ pregnant + glucose, data = train)
# Apply the fitted LDA model to the test set
pred5 <- predict(model5, test)
# Check the misclassification rate
misclass_rate5 <- sum(pred5$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate5
```
```{r LDA-plot, fig.width=8, fig.height=8}
# Visualize the LDA model
partimat(diabetes ~ pregnant + glucose, data = train, method="lda")
```

## Question 7
_Under the same training and test sets, if you switch to the quadratic discriminant analysis by using all the explanatory variables, does the misclassification rate get decreased or increased?_
```{r}
# Apply a quadratic discriminant analysis (QDA) model to the training set using all the explanatory variables
model6 <- qda(diabetes ~ ., data = train)
model6
```
```{r}
# Apply the fitted QDA model to the test set
pred6 <- predict(model6, test)
# Check the misclassification rate
misclass_rate6 <- sum(pred6$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate6
```
The misclassification rate of the QDA model is slightly higher than the misclassification rate of the LDA model. So, the LDA model is better than the QDA model.

Global conclusion: the best model is `model2`, which is a logistic regression model with significant only predictors.
The miscalsification rate of this model is `r misclass_rate2`.
And the equation of this model is:
```{r echo = FALSE, results = 'asis', message = FALSE, warning = FALSE}
library(texPreview)

tex_opts$set(
        density = 600  # High resolution LaTeX output
)

# Include HTML images when knitting with github_document
if (isTRUE(getOption('knitr.in.progress'))) {
  tex_opts$set(
          returnType = "html"
  )
}

# Don't use Helvetica in examples
# This could go in tex_opts$set, but it doesn't seem to actually get used there
usrPackages <- "\\renewcommand*\\familydefault{\\rmdefault}"

library('tidyverse')
# display the actual coefficients
equatiomatic::extract_eq(model2, use_coefs = TRUE) %>%
        tex_preview(usrPackages = usrPackages)
```

