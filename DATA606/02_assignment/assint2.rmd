---
title: 'Assignment 2'
author: 'Andrii Voitkiv'
date: "`r format(Sys.time(), '%a, %b %d, %Y')`"
output:
    github_document:
        math_method: webtex
        html_preview: false
        toc: true
        toc_depth: 2
---

```{r global_options}
knitr::opts_chunk$set(fig.path='Figs/')
```
## Question 1
Read the data “PimaIndiansDiabetes”, check the dimension of the dataset, the names of variables. Check how many classes there are (the variable “Diabetes”) and the class sizes. Check how R dummifies the “diabetes” variable. (3pt)
```{r}
# Read the data
library('mlbench')
data(PimaIndiansDiabetes)
# Check the dimension of the dataset
dim(PimaIndiansDiabetes)
# Check the names of variables
names(PimaIndiansDiabetes)
# Check how many classes there are (the variable “Diabetes”) and the class sizes
table(PimaIndiansDiabetes$diabetes)
```
The dataset has 768 observations and 9 variables.The names of variables are: pregnant, glucose, pressure, triceps, insulin, mass, pedigree, age and diabetes. There are two classes in the variable diabetes: "neg" and "pos". The class sizes are 500 and 268, respectively.

## Question 2
Split the dataset into two parts – training and test.The training part contains 400 individuals from the “neg” class and 200 units from the “pos” class. The test part contains the rest data.
```{r}
# Sort the dataset by the variable “diabetes”
PimaIndiansDiabetes <- PimaIndiansDiabetes[order(PimaIndiansDiabetes$diabetes),]
# Select 400 individuals from the “neg” class and 200 units from the “pos” class
train_neg <- PimaIndiansDiabetes[1:400,]
train_pos <- PimaIndiansDiabetes[501:700,]
# Combine the two datasets
train <- rbind(train_neg, train_pos)
# Select the rest data as the test part
test <- PimaIndiansDiabetes[-c(1:400, 501:700),]
# Check the dimension of the training and test parts
dim(train)[1]
dim(test)[1]
```

Let “diabetes” be the response variable, get a logistic regression model based on the training part using all the explanatory variables, which variables contribute to the “pos” result in a negative way?
```{r}
# Get a logistic regression model based on the training part using all the explanatory variables
model <- glm(diabetes ~ ., data = train, family = binomial)
# Check the coefficients
summary(model)
```
```{r}
# Check encoding of the variable “diabetes”
contrasts(PimaIndiansDiabetes$diabetes)
```
Conclusion: Default encoding of the variable “diabetes” is 1 for "pos". There are no statistically _significant_ variables that contribute to the “pos” result in a negative way. From the non-significant variables, the variables pressure, triceps and insulin contribute to the “pos” result in a negative way.


Check the “variance inflation factor” for each coefficient (e.g., using “vif” function from the package “car”). Do you detect multicollinearity?
```{r correlation-matrix}
# Plot the correlation matrix with the help of the ggally package
library('GGally')
library('ggplot2')
ggpairs(PimaIndiansDiabetes, lower = list(continuous = "smooth_loess", combo = "facethist", discrete = "facetbar", na = "na"))

```

```{r message=FALSE, warning=FALSE}
# Check the “variance inflation factor” for each coefficient using imcdiag function from the package "regclass"
library('regclass')
VIF(model)
```
The correlation matrix shows that there is no multicollinearity between the variables. The variance inflation factor is less than 5 for all the variables, so there is no multicollinearity.

## Question 3
Apply your fitted logistic regression model to the test set. Suppose a prediction of “pos” is made if P(Y=pos|X1,…,Xp)≥0.5
, check the misclassification rate (the ratio between the number of incorrect predictions and test size). (4pt)
```{r}
# Apply the fitted logistic regression model to the test set
pred <- predict(model, test, type = "response")
# Check the misclassification rate
misclass_rate <- sum(ifelse(pred >= 0.5, 'pos', 'neg') != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate
```

## Question 4
Let the significance level be 0.1, remove the explanatory variables whose coefficients are NOT significantly different from 0. Re-fit a logistic regression model and repeat Step 3 to calculate the misclassification rate. (3pt)
```{r}
# Check the sificance level of the coefficients with the significance level 0.1
summary(model)
```
With the significance level 0.1, the coefficients of the variables pregnant, glucose, pressure, mass, pedigree are significantly different from 0. So, we will keep them in the model and will drop the rest.

```{r}
# Re-fit a logistic regression model
model2 <- glm(diabetes ~ pregnant + glucose + pressure + mass + pedigree, data = train, family = binomial)
# Check the coefficients
summary(model2)
```

```{r}
# Apply the fitted logistic regression model to the test set
pred2 <- predict(model2, test, type = "response")
# Check the misclassification rate
misclass_rate2 <- sum(ifelse(pred2 >= 0.5, 'pos', 'neg') != test$diabetes)/dim(test)[1]
# Print the misclassification rates
misclass_rate2
```
If we compare the misclassification rates of the two models, we can see that the misclassification rate of the second model is slightly lower than the misclassification rate of the first model. So, the second model is better than the first one.

## Question 5
Under the same training and test sets, apply a linear discriminant analysis (LDA) model to the training set using all the explanatory variables. Apply this fitted LDA model to the test set, calculate the misclassification rate based on the LDA-predicted results and the actual results. (3pt)
```{r}
# Apply a linear discriminant analysis (LDA) model to the training set using all the explanatory variables
library('MASS')
model3 <- lda(diabetes ~ ., data = train)
model3
```
```{r}
# Apply the fitted LDA model to the test set
pred3 <- predict(model3, test)
# Check the misclassification rate
misclass_rate3 <- sum(pred3$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate3
```

## Question 6
Suppose now you only want to use two explanatory variables among “pregnant”, “glucose”, “mass” and “pedigree” to establish the LDA model. Which two variables can give you the lowest error rate based on the training set? After you get the LDA model, apply it to the test set, what is the misclassification rate? (3pt)
```{r}
# Check the misclassification rate for each pair of variables
variables <- c('pregnant', 'glucose', 'mass', 'pedigree')
misclass_result <- list(var1=c(), var2=c(), misclass_rate=c())

for (i in variables) {
  for (j in variables) {
    # Check if the pair is not the same
    # And if the pair is not already in the list
    if (i != j & !((i %in% misclass_result$var2) & (j %in% misclass_result$var1))) {
      pair <- c(i, j)
      model4 <- lda(reformulate(pair,"diabetes"), data = train)
      pred4 <- predict(model4, test)
      # Populate the list
      misclass_result$var1 <- c(misclass_result$var1, i)
      misclass_result$var2 <- c(misclass_result$var2, j)
      misclass_result$misclass_rate <- c(misclass_result$misclass_rate, sum(pred4$class != test$diabetes)/dim(test)[1])
    }
  }
}
# Transform the list to a data frame
misclass_result <- as.data.frame(misclass_result)
# Sort the data frame by the misclassification rate
misclass_result <- misclass_result[order(misclass_result$misclass_rate),]
# Print the data frame
misclass_result
```
The lowest error rate is 0.238. And it is achieved by the pair of variables “pregnant” and “glucose”.


```{r}
# Build the LDA model with the pair of variables that gives the lowest error rate
model5 <- lda(diabetes ~ pregnant + glucose, data = train)
# Apply the fitted LDA model to the test set
pred5 <- predict(model5, test)
# Check the misclassification rate
misclass_rate5 <- sum(pred5$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate5
```
```{r LDA-plot, fig.width=8, fig.height=8}
# Visualize the LDA model
library('klaR')
partimat(diabetes ~ pregnant + glucose, data = train, method="lda")
```

## Question 7
Under the same training and test sets, if you switch to the quadratic discriminant analysis by using all the explanatory variables, does the misclassification rate get decreased or increased?
```{r}
# Apply a quadratic discriminant analysis (QDA) model to the training set using all the explanatory variables
model6 <- qda(diabetes ~ ., data = train)
model6
```
```{r}
# Apply the fitted QDA model to the test set
pred6 <- predict(model6, test)
# Check the misclassification rate
misclass_rate6 <- sum(pred6$class != test$diabetes)/dim(test)[1]
# Print the misclassification rate
misclass_rate6
```
The misclassification rate of the QDA model is slightly higher than the misclassification rate of the LDA model. So, the LDA model is better than the QDA model.

```{r QDA-plot, fig.width=8, fig.height=8}
# Visualize the QDA model
library('klaR')
partimat(diabetes ~ ., data = train, method="qda")
```
